{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution1 Tfidf-linearSVM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2nKRdgtp6U",
        "colab_type": "code",
        "outputId": "a4375855-a03c-479b-9d2b-47489a8a1a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADJKj2lOXc1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install emoji unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7xs-VeGgM2L",
        "colab_type": "text"
      },
      "source": [
        "**preprocess data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAomXmWit8cx",
        "colab_type": "code",
        "outputId": "34ab9d4a-1582-4de7-9b05-ae673112ede6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "multiple_punctuation_pattern = re.compile(r\"([\\\"\\.\\?\\!\\,\\:\\;\\-])(?:[\\\"\\.\\?\\!\\,\\:\\;\\-]){1,}\")\n",
        "word_tokenizer = MWETokenizer(separator='')\n",
        "multiple_emoji_pattern = re.compile(u\"([\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\u00a9\"\n",
        "        u\"\\u00ae\"\n",
        "        u\"\\u2000-\\u3300\"\n",
        "        \"]){1,}\", flags= re.UNICODE )\n",
        "\n",
        "normalizer = {'òa': 'oà',\n",
        "              'óa': 'oá',\n",
        "              'ỏa': 'oả',\n",
        "              'õa': 'oã',\n",
        "              'ọa': 'oạ',\n",
        "              'òe': 'oè',\n",
        "              'óe': 'oé',\n",
        "              'ỏe': 'oẻ',\n",
        "              'õe': 'oẽ',\n",
        "              'ọe': 'oẹ',\n",
        "              'ùy': 'uỳ',\n",
        "              'úy': 'uý',\n",
        "              'ủy': 'uỷ',\n",
        "              'ũy': 'uỹ',\n",
        "              'ụy': 'uỵ',\n",
        "              'Ủy': 'Uỷ'}\n",
        "correct_mapping = {\n",
        "      \"m\": \"mình\",\n",
        "      \"mik\": \"mình\",\n",
        "      \"ko\": \"không\",\n",
        "      \"k\": \" không \",\n",
        "      \"kh\": \"không\",\n",
        "      \"khong\": \"không\",\n",
        "      \"kg\": \"không\",\n",
        "      \"khg\": \"không\",\n",
        "      \"tl\": \"trả lời\",\n",
        "      \"r\": \"rồi\",\n",
        "      \"ok\": \"tốt\",\n",
        "      \"dc\": \"được\",\n",
        "      \"vs\": \"với\",\n",
        "      \"đt\": \"điện thoại\",\n",
        "      \"thjk\": \"thích\",\n",
        "      \"thik\": \"thích\",\n",
        "      \"qá\": \"quá\",\n",
        "      \"trể\": \"trễ\",\n",
        "      \"bgjo\": \"bao giờ\",\n",
        "      \"''\": '\"',\n",
        "      \"``\": '\"'\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "\n",
        "  # for l in vn_location:\n",
        "  #   text = text.replace(l, ' location ')\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokmap(tok):\n",
        "  if tok.lower() in correct_mapping:\n",
        "      return correct_mapping[tok.lower()]\n",
        "  else:\n",
        "      return tok\n",
        "\n",
        "def preprocess(text):\n",
        "  global i\n",
        "  text = multiple_emoji_pattern.sub(r\"\\g<1> \", text) # \\g<1>\n",
        "  text = multiple_punctuation_pattern.sub(r\" \\g<1> \", text)\n",
        "  text = unicodedata.normalize(\"NFC\", text)\n",
        "  text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b(\\/)?', 'url', text)\n",
        "  text = re.sub(\"\\.\", \" . \", text)\n",
        "  text = re.sub(\"'\", \"' \", text)\n",
        "  text = re.sub('\"', '\" ', text)\n",
        "  text = re.sub('/', ' / ', text)\n",
        "  text = re.sub('-', ' - ', text)\n",
        "  text = re.sub(',', ' , ', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = normalize_text(text)\n",
        "  # text = re.sub(r'\\#[^\\s]+', ' hastag ', text)\n",
        "  text = re.sub(r'(|\\s)([\\d]+k)(\\s|$)', ' cureency_k ', text)\n",
        "  text = re.sub(r'(([\\d]{2,4}\\s){2,}([\\d]+)?|(09|01|[2|6|8|9]|03)+([0-9]{8})\\b)', ' phone_number ', text)\n",
        "  # text = re.sub(r'\\d', \"_digit\", text)\n",
        "  tokens = word_tokenizer.tokenize(word_tokenize(text))\n",
        "  tokens = list(map(tokmap, tokens))\n",
        "  # return tokens\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "print(preprocess('Địa chỉ : 103 d9 ngõ 63 , thái thịnh, đống đa, hà nội.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Địa chỉ : _digit_digit_digit d_digit ngõ _digit_digit , thái thịnh , đống đa , hà nội .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aix_308cedNf",
        "colab_type": "text"
      },
      "source": [
        "**Feature extraction model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4r55rTZWnjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import unidecode\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import emoji\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "\n",
        "class RemoveConsecutiveSpaces(BaseEstimator, TransformerMixin):\n",
        "    # def remove_consecutive_spaces(self, s):\n",
        "    #     return ' '.join(s.split())\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [re.sub(r'\\s+', ' ', s) for s in x] # self.remove_consecutive_spaces(s)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class RemovePunct(BaseEstimator, TransformerMixin):\n",
        "    non_special_chars = re.compile('[^A-Za-z0-9 ]+')\n",
        "\n",
        "    def remove_punct(self, s):\n",
        "        return re.sub(self.non_special_chars, '', s)\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [self.remove_punct(s) for s in x]\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "class RemoveTone(BaseEstimator, TransformerMixin):\n",
        "    def remove_tone(self, s):\n",
        "        return unidecode.unidecode(s)\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [self.remove_tone(s) for s in x]\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumWordsCharsFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_char(self, s):\n",
        "        return len(s)\n",
        "\n",
        "    def count_word(self, s):\n",
        "        return len(s.split())\n",
        "\n",
        "    def transform(self, x):\n",
        "        count_chars = sp.csr_matrix([self.count_char(s) for s in x], dtype=np.float64).transpose()\n",
        "        count_words = sp.csr_matrix([self.count_word(s) for s in x], dtype=np.float64).transpose()\n",
        "\n",
        "        return sp.hstack([count_chars, count_words])\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class ExclamationMarkFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_exclamation(self, s):\n",
        "        count = s.count('!') + s.count('?')\n",
        "        return count / (1 + len(s.split()))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_exclamation(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumCapitalLettersFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_upper(self, s):\n",
        "        n_uppers = sum(1 for c in s if c.isupper())\n",
        "        return n_uppers / (1 + len(s))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_upper(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumLowercaseLettersFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_lower(self, s):\n",
        "        n_lowers = sum(1 for c in s if c.islower())\n",
        "        return n_lowers / (1 + len(s))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_lower(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumPunctsFeature(BaseEstimator, TransformerMixin):\n",
        "    def count(self, l1, l2):\n",
        "        return sum([1 for x in l1 if x in l2])\n",
        "\n",
        "    def count_punct(self, s):\n",
        "        n_puncts = self.count(s, set(string.punctuation))\n",
        "        return n_puncts / (1 + len(s))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_punct(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "class NumHashTagFeature(BaseEstimator, TransformerMixin):\n",
        "  def count(self, s):\n",
        "    multiple_star = re.compile(r\"[\\#]+\")\n",
        "    n_hash = len(multiple_star.findall(s))\n",
        "    return n_hash / (1 + len(s.split() ))\n",
        "\n",
        "  def transform(self, x):\n",
        "    multiple_star = re.compile(r\"[\\#]+\")\n",
        "    counts = [ self.count(s) for s in x]\n",
        "    return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "  \n",
        "  def fit(self, x, y=None):\n",
        "    return self\n",
        "\n",
        "\n",
        "class NumEmojiFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_emoji(self, s):\n",
        "        emoji_list = []\n",
        "        for c in s:\n",
        "            if c in emoji.UNICODE_EMOJI:\n",
        "                emoji_list.append(c)\n",
        "        return len(emoji_list) / (1 + len(s.split()))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_emoji(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "feature_extraction = Pipeline([\n",
        "    ('remove_spaces', RemoveConsecutiveSpaces()),\n",
        "    ('features', FeatureUnion([\n",
        "        ('custom_features_pipeline', Pipeline([\n",
        "            ('custom_features', FeatureUnion([\n",
        "                ('f01', NumWordsCharsFeature()),\n",
        "                ('f02', NumCapitalLettersFeature()),\n",
        "                ('f03', ExclamationMarkFeature()),\n",
        "                ('f04', NumPunctsFeature()),\n",
        "                ('f05', NumLowercaseLettersFeature()),\n",
        "                ('f06', NumEmojiFeature()),\n",
        "            ], n_jobs=-1)),\n",
        "            ('scaler', StandardScaler(with_mean=False))\n",
        "        ])),\n",
        "        ('word_char_features_pipeline', Pipeline([\n",
        "            ('word_char_features', FeatureUnion([\n",
        "                ('with_tone', Pipeline([\n",
        "                    ('tf_idf_word', TfidfVectorizer(ngram_range=(1, 3), norm='l2', min_df=2)) # lowercase=False\n",
        "                ])),\n",
        "                ('with_tone_char', TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char')),\n",
        "                ('with_tone_char_wb', TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char_wb')),\n",
        "                ('without_tone', Pipeline([\n",
        "                    ('remove_tone', RemoveTone()),\n",
        "                    ('without_tone_features', FeatureUnion([\n",
        "                        ('tf_idf', Pipeline([\n",
        "                            ('word', TfidfVectorizer(ngram_range=(1, 3), norm='l2', min_df=2) )\n",
        "                        ])),\n",
        "                        ('tf_idf_char', TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char')),\n",
        "                        ('tf_idf_char_wb',\n",
        "                          TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char_wb'))\n",
        "                    ], n_jobs=-1))\n",
        "                ]))\n",
        "            ], n_jobs=-1))\n",
        "        ]))\n",
        "    ], n_jobs=-1)),\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTiPcpD33go",
        "colab_type": "text"
      },
      "source": [
        "**LOAD train data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9R5vQ1kywx3",
        "colab_type": "code",
        "outputId": "be20674d-d48b-486d-b4cb-a8dbdcc68f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import utils as skutils\n",
        "\n",
        "\n",
        "def maping_lable(lable):\n",
        "  if lable == '__label__post_khong_ban':\n",
        "    return 0\n",
        "  return 1\n",
        "\n",
        "\n",
        "def load_train_test():\n",
        "  with open('./sell_detection_train.v1.0.txt') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "    x_train = [t.split(' ', 1)[1] for t in lines]\n",
        "    x_train = [ preprocess(descriptions) for descriptions in x_train]\n",
        "\n",
        "    y_train = [t.split(' ', 1)[0] for t in lines]\n",
        "    y_train = list(map(maping_lable, y_train))\n",
        "\n",
        "  # with open('10kdigit_encode.txt') as f:\n",
        "  #   lines = f.read().strip().split('\\n')\n",
        "  #   x_test = [t.split(' ', 1)[1] for t in lines]\n",
        "  #   y_test = [t.split(' ', 1)[0] for t in lines]\n",
        "  #   y_test = list(map(maping_lable, y_test))\n",
        "  # x_train.extend(x_test)\n",
        "  # y_train.extend(y_test)\n",
        "  return skutils.Bunch(x_train=x_train, y_train=y_train) #, x_test=x_test, y_test=y_test)\n",
        "\n",
        "data = load_train_test()\n",
        "\n",
        "print(len(data.y_train), len(data.x_train) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSKiiL055Qx5",
        "colab_type": "text"
      },
      "source": [
        "**Load test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUJPjOp24tzQ",
        "colab_type": "code",
        "outputId": "77bbc7de-a7c8-42b4-8505-fb0e4c55fce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "with open('sell_detection_test.v1.0.txt') as f:\n",
        "    sell_detection_test = f.read().strip().split('\\n')\n",
        "    sell_detection_test = [ preprocess(e) for e in sell_detection_test]\n",
        "print(sell_detection_test[:10])\n",
        "print(len(sell_detection_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffBéo mà .', 'Minh la tai nguyen', 'Moi bat được con nay nhau được Hok', 'Bữa ăn sáng trên bầu trời', '❤ ️ 01 / 01 / 2013 - 01 / 01 / 2018 : Gặp được nhau đã là cái duyên , đến với nhau , bên cạnh nhau , quan tâm nhau . có lẽ là do số phận ! 5 năm thanh xuân có nhau . lãi được 1 cục 💎 P / s : Mỗi năm lãi đều như vậy thì lời to rồi 😍 # 5year_anniversary # HTloveMN # kếhoạch5nămlần1', 'Sơri Võ', 'ngu thoj chuc ka nha ngu ngon', 'Thực hiện chính sách đại lý mới nhằm chống bán phá giá để bảo vệ thương hiệu và các đại lý của Istitch . Kể từ nay Istitch chỉ bán cho 1 đại lý ở mỗi tỉnh thành . Các quý đại lý có nhu cầu xin vui lòng liên hệ email : thanhhongdo @ gmail . com để lấy báo giá riêng nhé ! Xin cám ơn .', 'Snowntown Sài Gòn ❤ ️⛄ ️❄ ️', 'ດົນໆໄປເທື່ອ . ມ່ວນຫຼາຍມື້ນີ້']\n",
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3SnUx2_y8Wp",
        "colab_type": "code",
        "outputId": "95c301c1-a3b8-4a61-9419-a8cc9297a1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_feature = feature_extraction.fit_transform(data.x_train)\n",
        "print(x_train_feature.shape)\n",
        "# x_test_feature = feature_extraction.transform(data.x_test)\n",
        "# print(x_test_feature.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 2134505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nt9zUzzVcYn",
        "colab_type": "code",
        "outputId": "f1c7e57e-4ccf-4a6c-9dbc-bd9e58341a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_feature = feature_extraction.transform(data.x_train)\n",
        "print(x_train_feature.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 2134505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsMRWG9nTqun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# with open ('feature_extraction.pickle', 'wb') as f:\n",
        "#   pickle.dump(feature_extraction, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID_D944CVCYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "\n",
        "# with open ('feature_extraction.pickle', 'rb') as f:\n",
        "#   feature_extraction = pickle.load(f) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-nhAVCJWmju",
        "colab_type": "code",
        "outputId": "201226b8-0a6c-40b4-dd7e-d13f060787a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test_feature = feature_extraction.transform(sell_detection_test)\n",
        "print(x_test_feature.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(98893, 2134505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMaFFIcxeZcp",
        "colab_type": "code",
        "outputId": "6851d15f-b2b0-4896-fe50-f3b701df9cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "linear_svm = LinearSVC()\n",
        "linear_svm.fit(x_train_feature, data.y_train)\n",
        "\n",
        "selection_model = SelectFromModel(linear_svm, prefit=True)\n",
        "\n",
        "X_train_selection = selection_model.transform(x_train_feature)\n",
        "X_test_selection = selection_model.transform(x_test_feature)\n",
        "print(X_train_selection.shape)\n",
        "print(X_test_selection.shape)\n",
        "\n",
        "\n",
        "LinearSVM = LinearSVC()\n",
        "LinearSVM.fit(X_train_selection, data.y_train)\n",
        "\n",
        "predicted = LinearSVM.predict(X_test_selection)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(60000, 560858)\n",
            "(98893, 560858)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rdDI3Kpil6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_maping_lable(lable):\n",
        "  if lable == 1:\n",
        "    return '__label__post_ban_hang'\n",
        "  return '__label__post_khong_ban'\n",
        "\n",
        "predicted_origin_lables = list(map(reverse_maping_lable, predicted))\n",
        "with open('sell_detection_team03_solution1.result.txt', 'w') as f:\n",
        "  f.write('\\n'.join(predicted_origin_lables))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}