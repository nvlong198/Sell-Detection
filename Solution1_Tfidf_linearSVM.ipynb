{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution1 Tfidf-linearSVM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2nKRdgtp6U",
        "colab_type": "code",
        "outputId": "a4375855-a03c-479b-9d2b-47489a8a1a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADJKj2lOXc1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install emoji unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7xs-VeGgM2L",
        "colab_type": "text"
      },
      "source": [
        "**preprocess data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAomXmWit8cx",
        "colab_type": "code",
        "outputId": "34ab9d4a-1582-4de7-9b05-ae673112ede6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "multiple_punctuation_pattern = re.compile(r\"([\\\"\\.\\?\\!\\,\\:\\;\\-])(?:[\\\"\\.\\?\\!\\,\\:\\;\\-]){1,}\")\n",
        "word_tokenizer = MWETokenizer(separator='')\n",
        "multiple_emoji_pattern = re.compile(u\"([\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\u00a9\"\n",
        "        u\"\\u00ae\"\n",
        "        u\"\\u2000-\\u3300\"\n",
        "        \"]){1,}\", flags= re.UNICODE )\n",
        "\n",
        "normalizer = {'√≤a': 'o√†',\n",
        "              '√≥a': 'o√°',\n",
        "              '·ªèa': 'o·∫£',\n",
        "              '√µa': 'o√£',\n",
        "              '·ªça': 'o·∫°',\n",
        "              '√≤e': 'o√®',\n",
        "              '√≥e': 'o√©',\n",
        "              '·ªèe': 'o·∫ª',\n",
        "              '√µe': 'o·∫Ω',\n",
        "              '·ªçe': 'o·∫π',\n",
        "              '√πy': 'u·ª≥',\n",
        "              '√∫y': 'u√Ω',\n",
        "              '·ªßy': 'u·ª∑',\n",
        "              '≈©y': 'u·ªπ',\n",
        "              '·ª•y': 'u·ªµ',\n",
        "              '·ª¶y': 'U·ª∑'}\n",
        "correct_mapping = {\n",
        "      \"m\": \"m√¨nh\",\n",
        "      \"mik\": \"m√¨nh\",\n",
        "      \"ko\": \"kh√¥ng\",\n",
        "      \"k\": \" kh√¥ng \",\n",
        "      \"kh\": \"kh√¥ng\",\n",
        "      \"khong\": \"kh√¥ng\",\n",
        "      \"kg\": \"kh√¥ng\",\n",
        "      \"khg\": \"kh√¥ng\",\n",
        "      \"tl\": \"tr·∫£ l·ªùi\",\n",
        "      \"r\": \"r·ªìi\",\n",
        "      \"ok\": \"t·ªët\",\n",
        "      \"dc\": \"ƒë∆∞·ª£c\",\n",
        "      \"vs\": \"v·ªõi\",\n",
        "      \"ƒët\": \"ƒëi·ªán tho·∫°i\",\n",
        "      \"thjk\": \"th√≠ch\",\n",
        "      \"thik\": \"th√≠ch\",\n",
        "      \"q√°\": \"qu√°\",\n",
        "      \"tr·ªÉ\": \"tr·ªÖ\",\n",
        "      \"bgjo\": \"bao gi·ªù\",\n",
        "      \"''\": '\"',\n",
        "      \"``\": '\"'\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "\n",
        "  # for l in vn_location:\n",
        "  #   text = text.replace(l, ' location ')\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokmap(tok):\n",
        "  if tok.lower() in correct_mapping:\n",
        "      return correct_mapping[tok.lower()]\n",
        "  else:\n",
        "      return tok\n",
        "\n",
        "def preprocess(text):\n",
        "  global i\n",
        "  text = multiple_emoji_pattern.sub(r\"\\g<1> \", text) # \\g<1>\n",
        "  text = multiple_punctuation_pattern.sub(r\" \\g<1> \", text)\n",
        "  text = unicodedata.normalize(\"NFC\", text)\n",
        "  text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b(\\/)?', 'url', text)\n",
        "  text = re.sub(\"\\.\", \" . \", text)\n",
        "  text = re.sub(\"'\", \"' \", text)\n",
        "  text = re.sub('\"', '\" ', text)\n",
        "  text = re.sub('/', ' / ', text)\n",
        "  text = re.sub('-', ' - ', text)\n",
        "  text = re.sub(',', ' , ', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = normalize_text(text)\n",
        "  # text = re.sub(r'\\#[^\\s]+', ' hastag ', text)\n",
        "  text = re.sub(r'(|\\s)([\\d]+k)(\\s|$)', ' cureency_k ', text)\n",
        "  text = re.sub(r'(([\\d]{2,4}\\s){2,}([\\d]+)?|(09|01|[2|6|8|9]|03)+([0-9]{8})\\b)', ' phone_number ', text)\n",
        "  # text = re.sub(r'\\d', \"_digit\", text)\n",
        "  tokens = word_tokenizer.tokenize(word_tokenize(text))\n",
        "  tokens = list(map(tokmap, tokens))\n",
        "  # return tokens\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "print(preprocess('ƒê·ªãa ch·ªâ : 103 d9 ng√µ 63 , th√°i th·ªãnh, ƒë·ªëng ƒëa, h√† n·ªôi.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "ƒê·ªãa ch·ªâ : _digit_digit_digit d_digit ng√µ _digit_digit , th√°i th·ªãnh , ƒë·ªëng ƒëa , h√† n·ªôi .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aix_308cedNf",
        "colab_type": "text"
      },
      "source": [
        "**Feature extraction model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4r55rTZWnjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import unidecode\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import emoji\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "\n",
        "class RemoveConsecutiveSpaces(BaseEstimator, TransformerMixin):\n",
        "    # def remove_consecutive_spaces(self, s):\n",
        "    #     return ' '.join(s.split())\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [re.sub(r'\\s+', ' ', s) for s in x] # self.remove_consecutive_spaces(s)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class RemovePunct(BaseEstimator, TransformerMixin):\n",
        "    non_special_chars = re.compile('[^A-Za-z0-9 ]+')\n",
        "\n",
        "    def remove_punct(self, s):\n",
        "        return re.sub(self.non_special_chars, '', s)\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [self.remove_punct(s) for s in x]\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "class RemoveTone(BaseEstimator, TransformerMixin):\n",
        "    def remove_tone(self, s):\n",
        "        return unidecode.unidecode(s)\n",
        "\n",
        "    def transform(self, x):\n",
        "        return [self.remove_tone(s) for s in x]\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumWordsCharsFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_char(self, s):\n",
        "        return len(s)\n",
        "\n",
        "    def count_word(self, s):\n",
        "        return len(s.split())\n",
        "\n",
        "    def transform(self, x):\n",
        "        count_chars = sp.csr_matrix([self.count_char(s) for s in x], dtype=np.float64).transpose()\n",
        "        count_words = sp.csr_matrix([self.count_word(s) for s in x], dtype=np.float64).transpose()\n",
        "\n",
        "        return sp.hstack([count_chars, count_words])\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class ExclamationMarkFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_exclamation(self, s):\n",
        "        count = s.count('!') + s.count('?')\n",
        "        return count / (1 + len(s.split()))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_exclamation(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumCapitalLettersFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_upper(self, s):\n",
        "        n_uppers = sum(1 for c in s if c.isupper())\n",
        "        return n_uppers / (1 + len(s))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_upper(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumLowercaseLettersFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_lower(self, s):\n",
        "        n_lowers = sum(1 for c in s if c.islower())\n",
        "        return n_lowers / (1 + len(s))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_lower(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "class NumPunctsFeature(BaseEstimator, TransformerMixin):\n",
        "    def count(self, l1, l2):\n",
        "        return sum([1 for x in l1 if x in l2])\n",
        "\n",
        "    def count_punct(self, s):\n",
        "        n_puncts = self.count(s, set(string.punctuation))\n",
        "        return n_puncts / (1 + len(s))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_punct(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "class NumHashTagFeature(BaseEstimator, TransformerMixin):\n",
        "  def count(self, s):\n",
        "    multiple_star = re.compile(r\"[\\#]+\")\n",
        "    n_hash = len(multiple_star.findall(s))\n",
        "    return n_hash / (1 + len(s.split() ))\n",
        "\n",
        "  def transform(self, x):\n",
        "    multiple_star = re.compile(r\"[\\#]+\")\n",
        "    counts = [ self.count(s) for s in x]\n",
        "    return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "  \n",
        "  def fit(self, x, y=None):\n",
        "    return self\n",
        "\n",
        "\n",
        "class NumEmojiFeature(BaseEstimator, TransformerMixin):\n",
        "    def count_emoji(self, s):\n",
        "        emoji_list = []\n",
        "        for c in s:\n",
        "            if c in emoji.UNICODE_EMOJI:\n",
        "                emoji_list.append(c)\n",
        "        return len(emoji_list) / (1 + len(s.split()))\n",
        "\n",
        "    def transform(self, x):\n",
        "        counts = [self.count_emoji(s) for s in x]\n",
        "        return sp.csr_matrix(counts, dtype=np.float64).transpose()\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "feature_extraction = Pipeline([\n",
        "    ('remove_spaces', RemoveConsecutiveSpaces()),\n",
        "    ('features', FeatureUnion([\n",
        "        ('custom_features_pipeline', Pipeline([\n",
        "            ('custom_features', FeatureUnion([\n",
        "                ('f01', NumWordsCharsFeature()),\n",
        "                ('f02', NumCapitalLettersFeature()),\n",
        "                ('f03', ExclamationMarkFeature()),\n",
        "                ('f04', NumPunctsFeature()),\n",
        "                ('f05', NumLowercaseLettersFeature()),\n",
        "                ('f06', NumEmojiFeature()),\n",
        "            ], n_jobs=-1)),\n",
        "            ('scaler', StandardScaler(with_mean=False))\n",
        "        ])),\n",
        "        ('word_char_features_pipeline', Pipeline([\n",
        "            ('word_char_features', FeatureUnion([\n",
        "                ('with_tone', Pipeline([\n",
        "                    ('tf_idf_word', TfidfVectorizer(ngram_range=(1, 3), norm='l2', min_df=2)) # lowercase=False\n",
        "                ])),\n",
        "                ('with_tone_char', TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char')),\n",
        "                ('with_tone_char_wb', TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char_wb')),\n",
        "                ('without_tone', Pipeline([\n",
        "                    ('remove_tone', RemoveTone()),\n",
        "                    ('without_tone_features', FeatureUnion([\n",
        "                        ('tf_idf', Pipeline([\n",
        "                            ('word', TfidfVectorizer(ngram_range=(1, 3), norm='l2', min_df=2) )\n",
        "                        ])),\n",
        "                        ('tf_idf_char', TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char')),\n",
        "                        ('tf_idf_char_wb',\n",
        "                          TfidfVectorizer(ngram_range=(1, 6), norm='l2', min_df=2, analyzer='char_wb'))\n",
        "                    ], n_jobs=-1))\n",
        "                ]))\n",
        "            ], n_jobs=-1))\n",
        "        ]))\n",
        "    ], n_jobs=-1)),\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTiPcpD33go",
        "colab_type": "text"
      },
      "source": [
        "**LOAD train data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9R5vQ1kywx3",
        "colab_type": "code",
        "outputId": "be20674d-d48b-486d-b4cb-a8dbdcc68f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import utils as skutils\n",
        "\n",
        "\n",
        "def maping_lable(lable):\n",
        "  if lable == '__label__post_khong_ban':\n",
        "    return 0\n",
        "  return 1\n",
        "\n",
        "\n",
        "def load_train_test():\n",
        "  with open('./sell_detection_train.v1.0.txt') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "    x_train = [t.split(' ', 1)[1] for t in lines]\n",
        "    x_train = [ preprocess(descriptions) for descriptions in x_train]\n",
        "\n",
        "    y_train = [t.split(' ', 1)[0] for t in lines]\n",
        "    y_train = list(map(maping_lable, y_train))\n",
        "\n",
        "  # with open('10kdigit_encode.txt') as f:\n",
        "  #   lines = f.read().strip().split('\\n')\n",
        "  #   x_test = [t.split(' ', 1)[1] for t in lines]\n",
        "  #   y_test = [t.split(' ', 1)[0] for t in lines]\n",
        "  #   y_test = list(map(maping_lable, y_test))\n",
        "  # x_train.extend(x_test)\n",
        "  # y_train.extend(y_test)\n",
        "  return skutils.Bunch(x_train=x_train, y_train=y_train) #, x_test=x_test, y_test=y_test)\n",
        "\n",
        "data = load_train_test()\n",
        "\n",
        "print(len(data.y_train), len(data.x_train) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSKiiL055Qx5",
        "colab_type": "text"
      },
      "source": [
        "**Load test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUJPjOp24tzQ",
        "colab_type": "code",
        "outputId": "77bbc7de-a7c8-42b4-8505-fb0e4c55fce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "with open('sell_detection_test.v1.0.txt') as f:\n",
        "    sell_detection_test = f.read().strip().split('\\n')\n",
        "    sell_detection_test = [ preprocess(e) for e in sell_detection_test]\n",
        "print(sell_detection_test[:10])\n",
        "print(len(sell_detection_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffB√©o m√† .', 'Minh la tai nguyen', 'Moi bat ƒë∆∞·ª£c con nay nhau ƒë∆∞·ª£c Hok', 'B·ªØa ƒÉn s√°ng tr√™n b·∫ßu tr·ªùi', '‚ù§ Ô∏è 01 / 01 / 2013 - 01 / 01 / 2018 : G·∫∑p ƒë∆∞·ª£c nhau ƒë√£ l√† c√°i duy√™n , ƒë·∫øn v·ªõi nhau , b√™n c·∫°nh nhau , quan t√¢m nhau . c√≥ l·∫Ω l√† do s·ªë ph·∫≠n ! 5 nƒÉm thanh xu√¢n c√≥ nhau . l√£i ƒë∆∞·ª£c 1 c·ª•c üíé P / s : M·ªói nƒÉm l√£i ƒë·ªÅu nh∆∞ v·∫≠y th√¨ l·ªùi to r·ªìi üòç # 5year_anniversary # HTloveMN # k·∫øho·∫°ch5nƒÉml·∫ßn1', 'S∆°ri V√µ', 'ngu thoj chuc ka nha ngu ngon', 'Th·ª±c hi·ªán ch√≠nh s√°ch ƒë·∫°i l√Ω m·ªõi nh·∫±m ch·ªëng b√°n ph√° gi√° ƒë·ªÉ b·∫£o v·ªá th∆∞∆°ng hi·ªáu v√† c√°c ƒë·∫°i l√Ω c·ªßa Istitch . K·ªÉ t·ª´ nay Istitch ch·ªâ b√°n cho 1 ƒë·∫°i l√Ω ·ªü m·ªói t·ªânh th√†nh . C√°c qu√Ω ƒë·∫°i l√Ω c√≥ nhu c·∫ßu xin vui l√≤ng li√™n h·ªá email : thanhhongdo @ gmail . com ƒë·ªÉ l·∫•y b√°o gi√° ri√™ng nh√© ! Xin c√°m ∆°n .', 'Snowntown S√†i G√≤n ‚ù§ Ô∏è‚õÑ Ô∏è‚ùÑ Ô∏è', '‡∫î‡∫ª‡∫ô‡ªÜ‡ªÑ‡∫õ‡ªÄ‡∫ó‡∫∑‡ªà‡∫≠ . ‡∫°‡ªà‡∫ß‡∫ô‡∫´‡∫º‡∫≤‡∫ç‡∫°‡∫∑‡ªâ‡∫ô‡∫µ‡ªâ']\n",
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3SnUx2_y8Wp",
        "colab_type": "code",
        "outputId": "95c301c1-a3b8-4a61-9419-a8cc9297a1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_feature = feature_extraction.fit_transform(data.x_train)\n",
        "print(x_train_feature.shape)\n",
        "# x_test_feature = feature_extraction.transform(data.x_test)\n",
        "# print(x_test_feature.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 2134505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nt9zUzzVcYn",
        "colab_type": "code",
        "outputId": "f1c7e57e-4ccf-4a6c-9dbc-bd9e58341a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_feature = feature_extraction.transform(data.x_train)\n",
        "print(x_train_feature.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 2134505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsMRWG9nTqun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# with open ('feature_extraction.pickle', 'wb') as f:\n",
        "#   pickle.dump(feature_extraction, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID_D944CVCYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "\n",
        "# with open ('feature_extraction.pickle', 'rb') as f:\n",
        "#   feature_extraction = pickle.load(f) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-nhAVCJWmju",
        "colab_type": "code",
        "outputId": "201226b8-0a6c-40b4-dd7e-d13f060787a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test_feature = feature_extraction.transform(sell_detection_test)\n",
        "print(x_test_feature.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(98893, 2134505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMaFFIcxeZcp",
        "colab_type": "code",
        "outputId": "6851d15f-b2b0-4896-fe50-f3b701df9cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "linear_svm = LinearSVC()\n",
        "linear_svm.fit(x_train_feature, data.y_train)\n",
        "\n",
        "selection_model = SelectFromModel(linear_svm, prefit=True)\n",
        "\n",
        "X_train_selection = selection_model.transform(x_train_feature)\n",
        "X_test_selection = selection_model.transform(x_test_feature)\n",
        "print(X_train_selection.shape)\n",
        "print(X_test_selection.shape)\n",
        "\n",
        "\n",
        "LinearSVM = LinearSVC()\n",
        "LinearSVM.fit(X_train_selection, data.y_train)\n",
        "\n",
        "predicted = LinearSVM.predict(X_test_selection)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(60000, 560858)\n",
            "(98893, 560858)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rdDI3Kpil6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_maping_lable(lable):\n",
        "  if lable == 1:\n",
        "    return '__label__post_ban_hang'\n",
        "  return '__label__post_khong_ban'\n",
        "\n",
        "predicted_origin_lables = list(map(reverse_maping_lable, predicted))\n",
        "with open('sell_detection_team03_solution1.result.txt', 'w') as f:\n",
        "  f.write('\\n'.join(predicted_origin_lables))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}