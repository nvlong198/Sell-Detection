{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution2 biLSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmqL1FNfVjtk",
        "colab_type": "code",
        "outputId": "a21e0449-094a-4aee-d3ae-31101847d859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehasnf4tVnOA",
        "colab_type": "code",
        "outputId": "ff458c8d-2a5a-4f55-84af-9c1bdd56e792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/KPDL/LSTM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/KPDL/LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R7X9FbgVxuF",
        "colab_type": "code",
        "outputId": "d17f6ee8-cd49-412c-a8d0-478211e0956a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ll"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 67385\n",
            "-rw------- 1 root 63461217 Nov 10 03:22 embedding_matrix.npz\n",
            "-rw------- 1 root     5454 Nov 10 03:24 Model1_LSTM.ipynb\n",
            "-rw------- 1 root  1010048 Nov 10 03:24 test_data.pkl\n",
            "-rw------- 1 root  2427710 Nov 10 03:22 tokenizer_60971_token.pickle\n",
            "-rw------- 1 root  2097152 Nov 10 03:24 train_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bli5u3xeZVaz",
        "colab_type": "text"
      },
      "source": [
        "**Preprocess, Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcyMjXteVqcO",
        "colab_type": "code",
        "outputId": "0f8620e1-7a12-4520-e6aa-f22f034917d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP0bc1BjVv87",
        "colab_type": "code",
        "outputId": "cd282104-4ed9-4268-caea-df1b098c2bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "multiple_punctuation_pattern = re.compile(r\"([\\\"\\.\\?\\!\\,\\:\\;\\-])(?:[\\\"\\.\\?\\!\\,\\:\\;\\-]){1,}\")\n",
        "word_tokenizer = MWETokenizer(separator='')\n",
        "multiple_emoji_pattern = re.compile(u\"([\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\u00a9\"\n",
        "        u\"\\u00ae\"\n",
        "        u\"\\u2000-\\u3300\"\n",
        "        \"]){1,}\", flags= re.UNICODE )\n",
        "\n",
        "normalizer = {'√≤a': 'o√†',\n",
        "              '√≥a': 'o√°',\n",
        "              '·ªèa': 'o·∫£',\n",
        "              '√µa': 'o√£',\n",
        "              '·ªça': 'o·∫°',\n",
        "              '√≤e': 'o√®',\n",
        "              '√≥e': 'o√©',\n",
        "              '·ªèe': 'o·∫ª',\n",
        "              '√µe': 'o·∫Ω',\n",
        "              '·ªçe': 'o·∫π',\n",
        "              '√πy': 'u·ª≥',\n",
        "              '√∫y': 'u√Ω',\n",
        "              '·ªßy': 'u·ª∑',\n",
        "              '≈©y': 'u·ªπ',\n",
        "              '·ª•y': 'u·ªµ',\n",
        "              '·ª¶y': 'U·ª∑'}\n",
        "correct_mapping = {\n",
        "      \"m\": \"m√¨nh\",\n",
        "      \"mik\": \"m√¨nh\",\n",
        "      \"ko\": \"kh√¥ng\",\n",
        "      \"k\": \" kh√¥ng \",\n",
        "      \"kh\": \"kh√¥ng\",\n",
        "      \"khong\": \"kh√¥ng\",\n",
        "      \"kg\": \"kh√¥ng\",\n",
        "      \"khg\": \"kh√¥ng\",\n",
        "      \"tl\": \"tr·∫£ l·ªùi\",\n",
        "      \"r\": \"r·ªìi\",\n",
        "      \"ok\": \"t·ªët\",\n",
        "      \"dc\": \"ƒë∆∞·ª£c\",\n",
        "      \"vs\": \"v·ªõi\",\n",
        "      \"ƒët\": \"ƒëi·ªán tho·∫°i\",\n",
        "      \"thjk\": \"th√≠ch\",\n",
        "      \"thik\": \"th√≠ch\",\n",
        "      \"q√°\": \"qu√°\",\n",
        "      \"tr·ªÉ\": \"tr·ªÖ\",\n",
        "      \"bgjo\": \"bao gi·ªù\",\n",
        "      \"''\": '\"',\n",
        "      \"``\": '\"'\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "\n",
        "  # for l in vn_location:\n",
        "  #   text = text.replace(l, ' location ')\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokmap(tok):\n",
        "  if tok.lower() in correct_mapping:\n",
        "      return correct_mapping[tok.lower()]\n",
        "  else:\n",
        "      return tok\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = multiple_emoji_pattern.sub(r\"\\g<1> \", text) # \\g<1>\n",
        "  text = multiple_punctuation_pattern.sub(r\" \\g<1> \", text)\n",
        "  text = unicodedata.normalize(\"NFC\", text)\n",
        "  text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b(\\/)?', 'url', text)\n",
        "  text = re.sub(\"\\.\", \" . \", text)\n",
        "  text = re.sub(\"'\", \"' \", text)\n",
        "  text = re.sub('\"', '\" ', text)\n",
        "  text = re.sub('/', ' / ', text)\n",
        "  text = re.sub('-', ' - ', text)\n",
        "  text = re.sub(',', ' , ', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = normalize_text(text)\n",
        "  # text = re.sub(r'\\#[^\\s]+', ' hastag ', text)\n",
        "  text = re.sub(r'(|\\s)([\\d]+k)(\\s|$)', ' cureency_k ', text)\n",
        "  text = re.sub(r'(([\\d]{2,4}\\s){2,}([\\d]+)?|(09|01|[2|6|8|9]|03)+([0-9]{8})\\b)', ' phone_number ', text)\n",
        "  # text = re.sub(r'\\d', \"_digit\", text)\n",
        "  tokens = word_tokenizer.tokenize(word_tokenize(text))\n",
        "  tokens = list(map(tokmap, tokens))\n",
        "  # return tokens\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "print(preprocess('ƒê·ªãa ch·ªâ : 103 d9 ng√µ 63 , th√°i th·ªãnh, ƒë·ªëng ƒëa, h√† n·ªôi.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "ƒë·ªãa ch·ªâ : 103 d9 ng√µ 63 , th√°i th·ªãnh , ƒë·ªëng ƒëa , h√† n·ªôi .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HJciVCNZdGv",
        "colab_type": "text"
      },
      "source": [
        "**Load training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y5-noJ13B8f",
        "colab_type": "code",
        "outputId": "050a5ffb-1d60-417e-ef4e-06ced7013b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('./sell_detection_train.v1.0.txt') as f:\n",
        "  xtrain = f.read().strip().split('\\n')\n",
        "  ytrain = [ line.split(' ',1)[0] for line in xtrain]\n",
        "  xtrain = [ line.split(' ',1)[1] for line in xtrain]\n",
        "  xtrain = [ preprocess(descriptions) for descriptions in xtrain]\n",
        "print(len(xtrain))\n",
        "print(xtrain[:10])\n",
        "print(ytrain[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "['gsu khsi nek dep gai hk cho y kirn yk nhoaz khsi huong nhiu', 'haiz . chan wa . hoc - hoc - nua - hoc mau', 'gƒÉng tay ng·∫≠p shop', 'üå∫ 2 si·ªÉu ph·∫©m x√¥ng b·∫Øc th·∫£o d∆∞·ª£c üå∫ ( c√¥ng d·ª•ng t·ª´ng lo·∫°i em k√®m d∆∞·ªõi h√¨nh nha kh√°ch y√™u ) üíß x√¥ng b·∫Øc ƒë·∫∑c tr·ªã m·ª•n ( d√πng cho kh√°ch nhi·ªÅu m·ª•n - tr·ªã ƒë∆∞·ª£c t·∫•t c·∫£ lo·∫°i m·ª•n ) üíß x√¥ng b·∫Øc tr·∫Øng da ( k·ªÅm nh·ªùn - tr·∫Øng m·ªãn da - lo·∫°i b·ªè m·ª•n ƒë·∫ßu ƒëen - m·ª•n c√°m ) üí´ lo·∫°i x√¥ng tr·∫Øng khi x√¥ng h∆°i xong kh√°ch c√≥ th·ªÉ d√πng n∆∞·ªõc th·∫£o d∆∞·ª£c ƒë·ªÉ t·∫Øm tr·∫Øng d∆∞·ª°ng da lu√¥n ·∫° . em b√°n m√°t tay 2 lo·∫°i n√†y l·∫Øm lu√¥n nha kh√°ch üíã nam n·ªØ s·ª≠ d·ª•ng ƒë·ªÅu ƒë∆∞·ª£c √† ‚ö° Ô∏è‚ö° Ô∏è‚ö° Ô∏èh√†ng x√¥ng nh√† em kh√¥ng pha t·∫°p , ƒë·∫ßy ƒë·ªß th√†nh ph·∫ßn , an to√†n cho da v√¨ l√† 100 % thi√™n nhi√™n : 3 √† m√† kh√°ch bi·∫øt t√°nh em ƒë√≥ gi·ªù lu√¥n b√°n h√†ng thi√™n nhi√™n d√≤iii ‚ù§ Ô∏è', 'n·ªïi nh·ªõ d√¢n tr√†o üòÇ', '\" hoi trai sum vay \"', 'th·∫≠t s·ª± jo n√†y m√¨nh r·∫•t bu·ªìn . c·ªë nh∆∞ng kh√¥ng th·ªÉ', 't·ªï qu·ªëc ch∆∞a bao gi·ªù thi√™ng li√™ng ƒë·∫øn th·∫ø .', '- ch·∫•t li·ªáu : da th·∫≠t , l√≥t da - m√†u : ƒëen , m·∫≠n - s35 - 39 form th∆∞·ªùng - gi√° : cureency_k - ƒë·ªãa ch·ªâ : s·ªë 1 ng√µ 850 ƒë∆∞·ªùng l√°ng - l√°ng th∆∞·ª£ng - ƒë·ªëng ƒëa - h√† n·ªôi li√™n h·ªá s·ªâ l·∫ª add zalo : phone_number', 'm·∫´u k·ªá trang tr√≠ m·ªõi th√≠ch h·ª£p cho ph√≤ng kh√°ch d√πng ƒë·ªÉ tr∆∞ng b√†y c√°c v·∫≠t l∆∞u ni·ªám , ho·∫∑c l√†m k·ªá s√°ch cho ph√≤ng l√†m vi·ªác . k√≠ch th∆∞·ªõc 160 x 80 cm ch·∫•t li·ªáu g·ªó th√¥ng t·ª± nhi√™n ho·∫∑c g·ªó mdf ch·ªëng ·∫©m s∆°n m√†u tu·ª≥ √Ω c√¥ng ngh·ªá cureency_k . gi√° sp : 490 . 000 ƒë m·∫´u do b√™n m√¨nh t·ª± thi·∫øt k·∫ø n√™n c√≥ th·ªÉ thay ƒë·ªïi tu·ª≥ theo s·ªü th√≠ch mn . lh : 0938 . 238 . 475 goden star h√¢n h·∫°nh !']\n",
            "['__label__post_khong_ban', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_ban_hang', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_ban_hang', '__label__post_ban_hang']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WdJbDck4AMH",
        "colab_type": "code",
        "outputId": "849b481b-299b-47da-aee3-f0ab2875bdff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def maping_lable(lable):\n",
        "  if lable == '__label__post_khong_ban':\n",
        "    return 0\n",
        "  return 1\n",
        "ytrain = list(map(maping_lable, ytrain))\n",
        "print(ytrain[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVRTsE4VfpRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.concat([train_data, test_data])\n",
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "print(len(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-mya4XoWM95",
        "colab_type": "text"
      },
      "source": [
        "**Load embedding_matrix + tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gnMO_ZWL5t",
        "colab_type": "code",
        "outputId": "ba4d834f-6fc2-40cc-cb0e-c344656790e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "embedding_matrix = np.load('embedding_matrix_91104_tokens.npz')['embeddings']\n",
        "print(type(embedding_matrix))\n",
        "print(len(embedding_matrix))\n",
        "\n",
        "with open(\"tokenizer_91104_tokens.pickle\", \"rb\") as input_file:\n",
        "    tokenizer = pickle.load(input_file)\n",
        "    print(len(tokenizer.word_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "91105\n",
            "91103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2PC-0RzZ6_l",
        "colab_type": "text"
      },
      "source": [
        "**Model setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZTqtWWU8v1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install Keras==2.3.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P15WFqVWkTQ",
        "colab_type": "code",
        "outputId": "3433003a-f314-49c6-eb2a-a2f57a36d976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import Constant\n",
        "\n",
        "print('number of words: ', len(tokenizer.word_counts))\n",
        "\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(xtrain) # data.x_train\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(xtrain[:10000]) # data.x_test\n",
        "\n",
        "maxlen = 180\n",
        "X_train = pad_sequences(list_tokenized_train , maxlen=maxlen, padding='post')\n",
        "Y_train = ytrain #data.y_train #train_data['labels']\n",
        "\n",
        "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')\n",
        "Y_test = ytrain[:10000] #test_data['labels']\n",
        "print('train-test:', len(X_train), len(X_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words:  91103\n",
            "train-test: 60000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOzt8ooyW251",
        "colab_type": "code",
        "outputId": "3589bf5e-eb6a-40e5-8e3e-f509780c65d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "# from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import Constant\n",
        "\n",
        "embed_size = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                    output_dim=embed_size,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                    trainable=True))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "# early_stop = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0, mode='max')\n",
        "# mcp_save = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
        "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='max')\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 4\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs )\n",
        "          #validation_data=(X_test, Y_test) ) #, callbacks=[mcp_save]) # validation_split=0.1, validation_data=(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 300)         27331500  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 64)          85248     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                1300      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 27,418,069\n",
            "Trainable params: 27,418,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk2RxYw4ajpG",
        "colab_type": "text"
      },
      "source": [
        "**Load test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0O3g9Usjj64",
        "colab_type": "code",
        "outputId": "2f0c547c-92a9-47ad-c148-5298491dcd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "with open('./sell_detection_test.v1.0.txt') as f:\n",
        "    sell_detection_test = f.read().strip().split('\\n')\n",
        "    sell_detection_test = [ preprocess(e) for e in sell_detection_test]\n",
        "print(sell_detection_test[:10])\n",
        "print(len(sell_detection_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffb√©o m√† .', 'minh la tai nguyen', 'moi bat ƒë∆∞·ª£c con nay nhau ƒë∆∞·ª£c hok', 'b·ªØa ƒÉn s√°ng tr√™n b·∫ßu tr·ªùi', '‚ù§ Ô∏è 01 / 01 / 2013 - 01 / 01 / 2018 : g·∫∑p ƒë∆∞·ª£c nhau ƒë√£ l√† c√°i duy√™n , ƒë·∫øn v·ªõi nhau , b√™n c·∫°nh nhau , quan t√¢m nhau . c√≥ l·∫Ω l√† do s·ªë ph·∫≠n ! 5 nƒÉm thanh xu√¢n c√≥ nhau . l√£i ƒë∆∞·ª£c 1 c·ª•c üíé p / s : m·ªói nƒÉm l√£i ƒë·ªÅu nh∆∞ v·∫≠y th√¨ l·ªùi to r·ªìi üòç # 5year_anniversary # htlovemn # k·∫øho·∫°ch5nƒÉml·∫ßn1', 's∆°ri v√µ', 'ngu thoj chuc ka nha ngu ngon', 'th·ª±c hi·ªán ch√≠nh s√°ch ƒë·∫°i l√Ω m·ªõi nh·∫±m ch·ªëng b√°n ph√° gi√° ƒë·ªÉ b·∫£o v·ªá th∆∞∆°ng hi·ªáu v√† c√°c ƒë·∫°i l√Ω c·ªßa istitch . k·ªÉ t·ª´ nay istitch ch·ªâ b√°n cho 1 ƒë·∫°i l√Ω ·ªü m·ªói t·ªânh th√†nh . c√°c qu√Ω ƒë·∫°i l√Ω c√≥ nhu c·∫ßu xin vui l√≤ng li√™n h·ªá email : thanhhongdo @ gmail . com ƒë·ªÉ l·∫•y b√°o gi√° ri√™ng nh√© ! xin c√°m ∆°n .', 'snowntown s√†i g√≤n ‚ù§ Ô∏è‚õÑ Ô∏è‚ùÑ Ô∏è', '‡∫î‡∫ª‡∫ô‡ªÜ‡ªÑ‡∫õ‡ªÄ‡∫ó‡∫∑‡ªà‡∫≠ . ‡∫°‡ªà‡∫ß‡∫ô‡∫´‡∫º‡∫≤‡∫ç‡∫°‡∫∑‡ªâ‡∫ô‡∫µ‡ªâ']\n",
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsGHCBD_krVb",
        "colab_type": "code",
        "outputId": "4971fb0d-80cf-48cf-c047-4ddbea6797bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list_tokenized_test = tokenizer.texts_to_sequences(sell_detection_test)\n",
        "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')\n",
        "print(len(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl4mF0F0ap9n",
        "colab_type": "text"
      },
      "source": [
        "**Generate prediction to file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MrHL_Rwk0EU",
        "colab_type": "code",
        "outputId": "ae980783-01f4-4756-e4a8-343544e5b9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = model.predict_classes(X_test)\n",
        "print(sum(predicted))\n",
        "def reverse_maping_lable(lable):\n",
        "  if lable == 1:\n",
        "    return '__label__post_ban_hang'\n",
        "  return '__label__post_khong_ban'\n",
        "\n",
        "predicted_origin_lables = list(map(reverse_maping_lable, predicted))\n",
        "with open('sell_detection_team03_solution2.result.txt', 'w') as f:\n",
        "  f.write('\\n'.join(predicted_origin_lables))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13839]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agJS9buxX-y_",
        "colab_type": "code",
        "outputId": "4ac7925c-1fa3-4dfd-c52a-5fa3bd38c78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# y_pred = model.predict_classes(X_test)\n",
        "# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "# print('F1-score: {0}'.format(f1_score(Y_test, y_pred)))\n",
        "# print('precision_score: {0}'.format(precision_score(Y_test, y_pred)))\n",
        "# print('recall_score: {0}'.format(recall_score(Y_test, y_pred)))\n",
        "# print('\\nConfusion matrix: \\n{}\\n'.format(confusion_matrix(Y_test, y_pred)))\n",
        "# print(classification_report(Y_test, y_pred, digits=6))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score: 0.9418472063854048\n",
            "precision_score: 0.9315789473684211\n",
            "recall_score: 0.9523443504996156\n",
            "\n",
            "Confusion matrix: \n",
            "[[8608   91]\n",
            " [  62 1239]]\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.992849  0.989539  0.991191      8699\n",
            "           1   0.931579  0.952344  0.941847      1301\n",
            "\n",
            "    accuracy                       0.984700     10000\n",
            "   macro avg   0.962214  0.970942  0.966519     10000\n",
            "weighted avg   0.984878  0.984700  0.984772     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc5R4EminITa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('predict_lable.txt', 'w') as f:\n",
        "  with open('../FastText Model/sell_detection_test.v1.0.txt') as f2:\n",
        "    sell_detection_test_file = f2.read().strip().split('\\n')\n",
        "  for i in range(len(predicted_origin_lables)):\n",
        "    f.write(predicted_origin_lables[i] + ' ' + sell_detection_test_file[i]+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIextjranjmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('../FastText Model/sell_detection_team03_solution1.result.txt') as f:\n",
        "  s1 = f.read().strip().split('\\n')\n",
        "with open('sell_detection_team03_solution2.1.result.txt') as f:\n",
        "  s2 = f.read().strip().split('\\n')\n",
        "with open('compare_svm_vs_lstm.txt', 'w') as f:\n",
        "  f.write('index, lablessvm , lableslstm2.1, test line\\n')\n",
        "  for i in range(len(sell_detection_test)):\n",
        "    if s1[i] != s2[i]:\n",
        "      f.write(str(i) + ' ' + s1[i] + ' ' + s2[i] + ' ' + sell_detection_test[i]+'\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwMSwjOg9Qxp",
        "colab_type": "code",
        "outputId": "614d9fc3-4e76-42b4-df81-8f415db9dd81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('compare_lstm2.1_svm.txt') as f:\n",
        "  print(len(f.read().split('\\n')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWLfDw33_aJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sell_detection_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}