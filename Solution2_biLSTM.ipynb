{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution2 biLSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmqL1FNfVjtk",
        "colab_type": "code",
        "outputId": "a21e0449-094a-4aee-d3ae-31101847d859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehasnf4tVnOA",
        "colab_type": "code",
        "outputId": "ff458c8d-2a5a-4f55-84af-9c1bdd56e792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/KPDL/LSTM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/KPDL/LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R7X9FbgVxuF",
        "colab_type": "code",
        "outputId": "d17f6ee8-cd49-412c-a8d0-478211e0956a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ll"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 67385\n",
            "-rw------- 1 root 63461217 Nov 10 03:22 embedding_matrix.npz\n",
            "-rw------- 1 root     5454 Nov 10 03:24 Model1_LSTM.ipynb\n",
            "-rw------- 1 root  1010048 Nov 10 03:24 test_data.pkl\n",
            "-rw------- 1 root  2427710 Nov 10 03:22 tokenizer_60971_token.pickle\n",
            "-rw------- 1 root  2097152 Nov 10 03:24 train_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bli5u3xeZVaz",
        "colab_type": "text"
      },
      "source": [
        "**Preprocess, Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcyMjXteVqcO",
        "colab_type": "code",
        "outputId": "0f8620e1-7a12-4520-e6aa-f22f034917d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP0bc1BjVv87",
        "colab_type": "code",
        "outputId": "cd282104-4ed9-4268-caea-df1b098c2bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "multiple_punctuation_pattern = re.compile(r\"([\\\"\\.\\?\\!\\,\\:\\;\\-])(?:[\\\"\\.\\?\\!\\,\\:\\;\\-]){1,}\")\n",
        "word_tokenizer = MWETokenizer(separator='')\n",
        "multiple_emoji_pattern = re.compile(u\"([\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\u00a9\"\n",
        "        u\"\\u00ae\"\n",
        "        u\"\\u2000-\\u3300\"\n",
        "        \"]){1,}\", flags= re.UNICODE )\n",
        "\n",
        "normalizer = {'òa': 'oà',\n",
        "              'óa': 'oá',\n",
        "              'ỏa': 'oả',\n",
        "              'õa': 'oã',\n",
        "              'ọa': 'oạ',\n",
        "              'òe': 'oè',\n",
        "              'óe': 'oé',\n",
        "              'ỏe': 'oẻ',\n",
        "              'õe': 'oẽ',\n",
        "              'ọe': 'oẹ',\n",
        "              'ùy': 'uỳ',\n",
        "              'úy': 'uý',\n",
        "              'ủy': 'uỷ',\n",
        "              'ũy': 'uỹ',\n",
        "              'ụy': 'uỵ',\n",
        "              'Ủy': 'Uỷ'}\n",
        "correct_mapping = {\n",
        "      \"m\": \"mình\",\n",
        "      \"mik\": \"mình\",\n",
        "      \"ko\": \"không\",\n",
        "      \"k\": \" không \",\n",
        "      \"kh\": \"không\",\n",
        "      \"khong\": \"không\",\n",
        "      \"kg\": \"không\",\n",
        "      \"khg\": \"không\",\n",
        "      \"tl\": \"trả lời\",\n",
        "      \"r\": \"rồi\",\n",
        "      \"ok\": \"tốt\",\n",
        "      \"dc\": \"được\",\n",
        "      \"vs\": \"với\",\n",
        "      \"đt\": \"điện thoại\",\n",
        "      \"thjk\": \"thích\",\n",
        "      \"thik\": \"thích\",\n",
        "      \"qá\": \"quá\",\n",
        "      \"trể\": \"trễ\",\n",
        "      \"bgjo\": \"bao giờ\",\n",
        "      \"''\": '\"',\n",
        "      \"``\": '\"'\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "\n",
        "  # for l in vn_location:\n",
        "  #   text = text.replace(l, ' location ')\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokmap(tok):\n",
        "  if tok.lower() in correct_mapping:\n",
        "      return correct_mapping[tok.lower()]\n",
        "  else:\n",
        "      return tok\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = multiple_emoji_pattern.sub(r\"\\g<1> \", text) # \\g<1>\n",
        "  text = multiple_punctuation_pattern.sub(r\" \\g<1> \", text)\n",
        "  text = unicodedata.normalize(\"NFC\", text)\n",
        "  text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b(\\/)?', 'url', text)\n",
        "  text = re.sub(\"\\.\", \" . \", text)\n",
        "  text = re.sub(\"'\", \"' \", text)\n",
        "  text = re.sub('\"', '\" ', text)\n",
        "  text = re.sub('/', ' / ', text)\n",
        "  text = re.sub('-', ' - ', text)\n",
        "  text = re.sub(',', ' , ', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = normalize_text(text)\n",
        "  # text = re.sub(r'\\#[^\\s]+', ' hastag ', text)\n",
        "  text = re.sub(r'(|\\s)([\\d]+k)(\\s|$)', ' cureency_k ', text)\n",
        "  text = re.sub(r'(([\\d]{2,4}\\s){2,}([\\d]+)?|(09|01|[2|6|8|9]|03)+([0-9]{8})\\b)', ' phone_number ', text)\n",
        "  # text = re.sub(r'\\d', \"_digit\", text)\n",
        "  tokens = word_tokenizer.tokenize(word_tokenize(text))\n",
        "  tokens = list(map(tokmap, tokens))\n",
        "  # return tokens\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "print(preprocess('Địa chỉ : 103 d9 ngõ 63 , thái thịnh, đống đa, hà nội.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "địa chỉ : 103 d9 ngõ 63 , thái thịnh , đống đa , hà nội .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HJciVCNZdGv",
        "colab_type": "text"
      },
      "source": [
        "**Load training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y5-noJ13B8f",
        "colab_type": "code",
        "outputId": "050a5ffb-1d60-417e-ef4e-06ced7013b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('./sell_detection_train.v1.0.txt') as f:\n",
        "  xtrain = f.read().strip().split('\\n')\n",
        "  ytrain = [ line.split(' ',1)[0] for line in xtrain]\n",
        "  xtrain = [ line.split(' ',1)[1] for line in xtrain]\n",
        "  xtrain = [ preprocess(descriptions) for descriptions in xtrain]\n",
        "print(len(xtrain))\n",
        "print(xtrain[:10])\n",
        "print(ytrain[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "['gsu khsi nek dep gai hk cho y kirn yk nhoaz khsi huong nhiu', 'haiz . chan wa . hoc - hoc - nua - hoc mau', 'găng tay ngập shop', '🌺 2 siểu phẩm xông bắc thảo dược 🌺 ( công dụng từng loại em kèm dưới hình nha khách yêu ) 💧 xông bắc đặc trị mụn ( dùng cho khách nhiều mụn - trị được tất cả loại mụn ) 💧 xông bắc trắng da ( kềm nhờn - trắng mịn da - loại bỏ mụn đầu đen - mụn cám ) 💫 loại xông trắng khi xông hơi xong khách có thể dùng nước thảo dược để tắm trắng dưỡng da luôn ạ . em bán mát tay 2 loại này lắm luôn nha khách 💋 nam nữ sử dụng đều được à ⚡ ️⚡ ️⚡ ️hàng xông nhà em không pha tạp , đầy đủ thành phần , an toàn cho da vì là 100 % thiên nhiên : 3 à mà khách biết tánh em đó giờ luôn bán hàng thiên nhiên dòiii ❤ ️', 'nổi nhớ dân trào 😂', '\" hoi trai sum vay \"', 'thật sự jo này mình rất buồn . cố nhưng không thể', 'tổ quốc chưa bao giờ thiêng liêng đến thế .', '- chất liệu : da thật , lót da - màu : đen , mận - s35 - 39 form thường - giá : cureency_k - địa chỉ : số 1 ngõ 850 đường láng - láng thượng - đống đa - hà nội liên hệ sỉ lẻ add zalo : phone_number', 'mẫu kệ trang trí mới thích hợp cho phòng khách dùng để trưng bày các vật lưu niệm , hoặc làm kệ sách cho phòng làm việc . kích thước 160 x 80 cm chất liệu gỗ thông tự nhiên hoặc gỗ mdf chống ẩm sơn màu tuỳ ý công nghệ cureency_k . giá sp : 490 . 000 đ mẫu do bên mình tự thiết kế nên có thể thay đổi tuỳ theo sở thích mn . lh : 0938 . 238 . 475 goden star hân hạnh !']\n",
            "['__label__post_khong_ban', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_ban_hang', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_khong_ban', '__label__post_ban_hang', '__label__post_ban_hang']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WdJbDck4AMH",
        "colab_type": "code",
        "outputId": "849b481b-299b-47da-aee3-f0ab2875bdff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def maping_lable(lable):\n",
        "  if lable == '__label__post_khong_ban':\n",
        "    return 0\n",
        "  return 1\n",
        "ytrain = list(map(maping_lable, ytrain))\n",
        "print(ytrain[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVRTsE4VfpRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.concat([train_data, test_data])\n",
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "print(len(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-mya4XoWM95",
        "colab_type": "text"
      },
      "source": [
        "**Load embedding_matrix + tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gnMO_ZWL5t",
        "colab_type": "code",
        "outputId": "ba4d834f-6fc2-40cc-cb0e-c344656790e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "embedding_matrix = np.load('embedding_matrix_91104_tokens.npz')['embeddings']\n",
        "print(type(embedding_matrix))\n",
        "print(len(embedding_matrix))\n",
        "\n",
        "with open(\"tokenizer_91104_tokens.pickle\", \"rb\") as input_file:\n",
        "    tokenizer = pickle.load(input_file)\n",
        "    print(len(tokenizer.word_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "91105\n",
            "91103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2PC-0RzZ6_l",
        "colab_type": "text"
      },
      "source": [
        "**Model setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZTqtWWU8v1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install Keras==2.3.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P15WFqVWkTQ",
        "colab_type": "code",
        "outputId": "3433003a-f314-49c6-eb2a-a2f57a36d976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import Constant\n",
        "\n",
        "print('number of words: ', len(tokenizer.word_counts))\n",
        "\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(xtrain) # data.x_train\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(xtrain[:10000]) # data.x_test\n",
        "\n",
        "maxlen = 180\n",
        "X_train = pad_sequences(list_tokenized_train , maxlen=maxlen, padding='post')\n",
        "Y_train = ytrain #data.y_train #train_data['labels']\n",
        "\n",
        "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')\n",
        "Y_test = ytrain[:10000] #test_data['labels']\n",
        "print('train-test:', len(X_train), len(X_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words:  91103\n",
            "train-test: 60000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOzt8ooyW251",
        "colab_type": "code",
        "outputId": "3589bf5e-eb6a-40e5-8e3e-f509780c65d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "# from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import Constant\n",
        "\n",
        "embed_size = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                    output_dim=embed_size,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                    trainable=True))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "# early_stop = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0, mode='max')\n",
        "# mcp_save = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
        "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='max')\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 4\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs )\n",
        "          #validation_data=(X_test, Y_test) ) #, callbacks=[mcp_save]) # validation_split=0.1, validation_data=(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 300)         27331500  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 64)          85248     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                1300      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 27,418,069\n",
            "Trainable params: 27,418,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk2RxYw4ajpG",
        "colab_type": "text"
      },
      "source": [
        "**Load test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0O3g9Usjj64",
        "colab_type": "code",
        "outputId": "2f0c547c-92a9-47ad-c148-5298491dcd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "with open('./sell_detection_test.v1.0.txt') as f:\n",
        "    sell_detection_test = f.read().strip().split('\\n')\n",
        "    sell_detection_test = [ preprocess(e) for e in sell_detection_test]\n",
        "print(sell_detection_test[:10])\n",
        "print(len(sell_detection_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffbéo mà .', 'minh la tai nguyen', 'moi bat được con nay nhau được hok', 'bữa ăn sáng trên bầu trời', '❤ ️ 01 / 01 / 2013 - 01 / 01 / 2018 : gặp được nhau đã là cái duyên , đến với nhau , bên cạnh nhau , quan tâm nhau . có lẽ là do số phận ! 5 năm thanh xuân có nhau . lãi được 1 cục 💎 p / s : mỗi năm lãi đều như vậy thì lời to rồi 😍 # 5year_anniversary # htlovemn # kếhoạch5nămlần1', 'sơri võ', 'ngu thoj chuc ka nha ngu ngon', 'thực hiện chính sách đại lý mới nhằm chống bán phá giá để bảo vệ thương hiệu và các đại lý của istitch . kể từ nay istitch chỉ bán cho 1 đại lý ở mỗi tỉnh thành . các quý đại lý có nhu cầu xin vui lòng liên hệ email : thanhhongdo @ gmail . com để lấy báo giá riêng nhé ! xin cám ơn .', 'snowntown sài gòn ❤ ️⛄ ️❄ ️', 'ດົນໆໄປເທື່ອ . ມ່ວນຫຼາຍມື້ນີ້']\n",
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsGHCBD_krVb",
        "colab_type": "code",
        "outputId": "4971fb0d-80cf-48cf-c047-4ddbea6797bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list_tokenized_test = tokenizer.texts_to_sequences(sell_detection_test)\n",
        "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')\n",
        "print(len(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl4mF0F0ap9n",
        "colab_type": "text"
      },
      "source": [
        "**Generate prediction to file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MrHL_Rwk0EU",
        "colab_type": "code",
        "outputId": "ae980783-01f4-4756-e4a8-343544e5b9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = model.predict_classes(X_test)\n",
        "print(sum(predicted))\n",
        "def reverse_maping_lable(lable):\n",
        "  if lable == 1:\n",
        "    return '__label__post_ban_hang'\n",
        "  return '__label__post_khong_ban'\n",
        "\n",
        "predicted_origin_lables = list(map(reverse_maping_lable, predicted))\n",
        "with open('sell_detection_team03_solution2.result.txt', 'w') as f:\n",
        "  f.write('\\n'.join(predicted_origin_lables))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13839]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agJS9buxX-y_",
        "colab_type": "code",
        "outputId": "4ac7925c-1fa3-4dfd-c52a-5fa3bd38c78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# y_pred = model.predict_classes(X_test)\n",
        "# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "# print('F1-score: {0}'.format(f1_score(Y_test, y_pred)))\n",
        "# print('precision_score: {0}'.format(precision_score(Y_test, y_pred)))\n",
        "# print('recall_score: {0}'.format(recall_score(Y_test, y_pred)))\n",
        "# print('\\nConfusion matrix: \\n{}\\n'.format(confusion_matrix(Y_test, y_pred)))\n",
        "# print(classification_report(Y_test, y_pred, digits=6))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score: 0.9418472063854048\n",
            "precision_score: 0.9315789473684211\n",
            "recall_score: 0.9523443504996156\n",
            "\n",
            "Confusion matrix: \n",
            "[[8608   91]\n",
            " [  62 1239]]\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.992849  0.989539  0.991191      8699\n",
            "           1   0.931579  0.952344  0.941847      1301\n",
            "\n",
            "    accuracy                       0.984700     10000\n",
            "   macro avg   0.962214  0.970942  0.966519     10000\n",
            "weighted avg   0.984878  0.984700  0.984772     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc5R4EminITa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('predict_lable.txt', 'w') as f:\n",
        "  with open('../FastText Model/sell_detection_test.v1.0.txt') as f2:\n",
        "    sell_detection_test_file = f2.read().strip().split('\\n')\n",
        "  for i in range(len(predicted_origin_lables)):\n",
        "    f.write(predicted_origin_lables[i] + ' ' + sell_detection_test_file[i]+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIextjranjmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('../FastText Model/sell_detection_team03_solution1.result.txt') as f:\n",
        "  s1 = f.read().strip().split('\\n')\n",
        "with open('sell_detection_team03_solution2.1.result.txt') as f:\n",
        "  s2 = f.read().strip().split('\\n')\n",
        "with open('compare_svm_vs_lstm.txt', 'w') as f:\n",
        "  f.write('index, lablessvm , lableslstm2.1, test line\\n')\n",
        "  for i in range(len(sell_detection_test)):\n",
        "    if s1[i] != s2[i]:\n",
        "      f.write(str(i) + ' ' + s1[i] + ' ' + s2[i] + ' ' + sell_detection_test[i]+'\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwMSwjOg9Qxp",
        "colab_type": "code",
        "outputId": "614d9fc3-4e76-42b4-df81-8f415db9dd81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('compare_lstm2.1_svm.txt') as f:\n",
        "  print(len(f.read().split('\\n')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWLfDw33_aJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sell_detection_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}