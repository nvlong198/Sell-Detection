{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution3 FastText-Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2nKRdgtp6U",
        "colab_type": "code",
        "outputId": "a4375855-a03c-479b-9d2b-47489a8a1a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRiHqKIft33h",
        "colab_type": "code",
        "outputId": "03ce08af-9ac8-4590-8297-9ef1b7c72509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/KPDL/FastText Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/KPDL/FastText Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADJKj2lOXc1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install emoji unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXNZ787KjiSs",
        "colab_type": "text"
      },
      "source": [
        "**Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAomXmWit8cx",
        "colab_type": "code",
        "outputId": "34ab9d4a-1582-4de7-9b05-ae673112ede6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "multiple_punctuation_pattern = re.compile(r\"([\\\"\\.\\?\\!\\,\\:\\;\\-])(?:[\\\"\\.\\?\\!\\,\\:\\;\\-]){1,}\")\n",
        "word_tokenizer = MWETokenizer(separator='')\n",
        "multiple_emoji_pattern = re.compile(u\"([\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\u00a9\"\n",
        "        u\"\\u00ae\"\n",
        "        u\"\\u2000-\\u3300\"\n",
        "        \"]){1,}\", flags= re.UNICODE )\n",
        "\n",
        "normalizer = {'òa': 'oà',\n",
        "              'óa': 'oá',\n",
        "              'ỏa': 'oả',\n",
        "              'õa': 'oã',\n",
        "              'ọa': 'oạ',\n",
        "              'òe': 'oè',\n",
        "              'óe': 'oé',\n",
        "              'ỏe': 'oẻ',\n",
        "              'õe': 'oẽ',\n",
        "              'ọe': 'oẹ',\n",
        "              'ùy': 'uỳ',\n",
        "              'úy': 'uý',\n",
        "              'ủy': 'uỷ',\n",
        "              'ũy': 'uỹ',\n",
        "              'ụy': 'uỵ',\n",
        "              'Ủy': 'Uỷ'}\n",
        "correct_mapping = {\n",
        "      \"m\": \"mình\",\n",
        "      \"mik\": \"mình\",\n",
        "      \"ko\": \"không\",\n",
        "      \"k\": \" không \",\n",
        "      \"kh\": \"không\",\n",
        "      \"khong\": \"không\",\n",
        "      \"kg\": \"không\",\n",
        "      \"khg\": \"không\",\n",
        "      \"tl\": \"trả lời\",\n",
        "      \"r\": \"rồi\",\n",
        "      \"ok\": \"tốt\",\n",
        "      \"dc\": \"được\",\n",
        "      \"vs\": \"với\",\n",
        "      \"đt\": \"điện thoại\",\n",
        "      \"thjk\": \"thích\",\n",
        "      \"thik\": \"thích\",\n",
        "      \"qá\": \"quá\",\n",
        "      \"trể\": \"trễ\",\n",
        "      \"bgjo\": \"bao giờ\",\n",
        "      \"''\": '\"',\n",
        "      \"``\": '\"'\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "\n",
        "  # for l in vn_location:\n",
        "  #   text = text.replace(l, ' location ')\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokmap(tok):\n",
        "  if tok.lower() in correct_mapping:\n",
        "      return correct_mapping[tok.lower()]\n",
        "  else:\n",
        "      return tok\n",
        "\n",
        "def preprocess(text):\n",
        "  global i\n",
        "  text = multiple_emoji_pattern.sub(r\"\\g<1> \", text) # \\g<1>\n",
        "  text = multiple_punctuation_pattern.sub(r\" \\g<1> \", text)\n",
        "  text = unicodedata.normalize(\"NFC\", text)\n",
        "  text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b(\\/)?', 'url', text)\n",
        "  text = re.sub(\"\\.\", \" . \", text)\n",
        "  text = re.sub(\"'\", \"' \", text)\n",
        "  text = re.sub('\"', '\" ', text)\n",
        "  text = re.sub('/', ' / ', text)\n",
        "  text = re.sub('-', ' - ', text)\n",
        "  text = re.sub(',', ' , ', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = normalize_text(text)\n",
        "  # text = re.sub(r'\\#[^\\s]+', ' hastag ', text)\n",
        "  text = re.sub(r'(|\\s)([\\d]+k)(\\s|$)', ' cureency_k ', text)\n",
        "  text = re.sub(r'(([\\d]{2,4}\\s){2,}([\\d]+)?|(09|01|[2|6|8|9]|03)+([0-9]{8})\\b)', ' phone_number ', text)\n",
        "  text = re.sub(r'\\d', \"_digit\", text)\n",
        "  tokens = word_tokenizer.tokenize(word_tokenize(text))\n",
        "  tokens = list(map(tokmap, tokens))\n",
        "  # return tokens\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "print(preprocess('Địa chỉ : 103 d9 ngõ 63 , thái thịnh, đống đa, hà nội.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Địa chỉ : _digit_digit_digit d_digit ngõ _digit_digit , thái thịnh , đống đa , hà nội .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSKiiL055Qx5",
        "colab_type": "text"
      },
      "source": [
        "# **preprocess FastText tranining & testing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VvzR9Fnq-Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./sell_detection_test.v1.0.txt') as f:\n",
        "    sell_detection_test = f.read().strip().split('\\n')\n",
        "    sell_detection_test = [ preprocess(e) for e in sell_detection_test]\n",
        "print(sell_detection_test[:10])\n",
        "print(len(sell_detection_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq6kmf5rsxHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sell_detection_fasttext_test.txt', 'w') as f:\n",
        "  f.write('\\n'.join(sell_detection_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6hGYvkLswVe",
        "colab_type": "code",
        "outputId": "41103beb-ae53-44cb-9898-eeaedd23d74c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('./sell_detection_train.v1.0.txt') as f:\n",
        "  sell_detection_train = f.read().strip().split('\\n')\n",
        "  sell_detection_train = [ line.split(' ',1) for line in sell_detection_train]\n",
        "  sell_detection_train = [ [lables, preprocess(descriptions)] for lables, descriptions in sell_detection_train]\n",
        "  sell_detection_train = [ ' '.join(e) for e in sell_detection_train]\n",
        "print(sell_detection_train[:2])\n",
        "with open('sell_detection_fasttext_train.txt', 'w') as f:\n",
        "  f.write('\\n'.join(sell_detection_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__label__post_khong_ban Gsu khsi nek dep gai hk cho y kirn yk nhoaz khsi huong nhiu', '__label__post_khong_ban Haiz . chan wa . hoc - hoc - nua - hoc mau']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWWNqkp6lGv9",
        "colab_type": "text"
      },
      "source": [
        "# **FastText Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr2qfDaOq2Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaqggOd1Fkg",
        "colab_type": "code",
        "outputId": "cc0c91ff-e838-47dd-a39d-1fbc73eb74c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import fasttext\n",
        "\n",
        "#Train the model\n",
        "model = fasttext.train_supervised(input=\"sell_detection_fasttext_train.txt\", lr=0.22, wordNgrams=3) #  epoch=5)# dim=10)\n",
        "# model.save_model(\"sell_dectection_60K_fasttext.bin\")\n",
        "prediction = []\n",
        "\n",
        "for i in range(len(sell_detection_test)):\n",
        "  prediction.append(model.predict(sell_detection_test[i])[0][0] )\n",
        "print(len(prediction))\n",
        "\n",
        "# def pre(pre):\n",
        "#   if pre == '__label__post_khong_ban':\n",
        "#     return 0\n",
        "#   return 1\n",
        "\n",
        "# prediction = list(map(pre, prediction))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjh18JT2lM90",
        "colab_type": "text"
      },
      "source": [
        "write solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTsQtt1xuiLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sell_detection_team03_solution3.result.txt', 'w') as f:\n",
        "  f.write('\\n'.join(prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}